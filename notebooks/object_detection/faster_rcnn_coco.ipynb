{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3320239",
   "metadata": {},
   "source": [
    "# Faster R-CNN on Coco\n",
    "# ðŸš€ Faster R-CNNï¼šç›®æ ‡æ£€æµ‹çš„é‡Œç¨‹ç¢‘ä¹‹ä½œ\n",
    "\n",
    "Faster R-CNN æ˜¯ç›®æ ‡æ£€æµ‹é¢†åŸŸé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œï¼Œç”± Shaoqing Renã€Kaiming Heã€Ross Girshick å’Œ Jian Sun åœ¨ 2015 å¹´æå‡ºï¼ˆè®ºæ–‡æ ‡é¢˜ï¼š[*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*](https://arxiv.org/abs/1506.01497)ï¼‰ã€‚å®ƒåœ¨ R-CNN ç³»åˆ—ä¸­æ‰¿å‰å¯åŽï¼Œè§£å†³äº†æ­¤å‰æ–¹æ³•çš„é€Ÿåº¦ç“¶é¢ˆï¼ŒåŒæ—¶ä¿æŒäº†é«˜ç²¾åº¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ ä¸€ã€æ ¸å¿ƒåˆ›æ–°ç‚¹\n",
    "\n",
    "### 1. å¼•å…¥ Region Proposal Network (RPN) â€”â€” é©å‘½æ€§çš„åŒºåŸŸæè®®æœºåˆ¶\n",
    "\n",
    "- **å–ä»£ Selective Search**ï¼šåœ¨ Fast R-CNN ä¸­ï¼ŒåŒºåŸŸæè®®ä»ä¾èµ–å¤–éƒ¨ç®—æ³•ï¼ˆå¦‚ Selective Searchï¼‰ï¼Œé€Ÿåº¦æ…¢ä¸”ä¸Žç½‘ç»œåˆ†ç¦»ã€‚\n",
    "- **RPN æ˜¯å…¨å·ç§¯ç½‘ç»œ**ï¼šä¸Žæ£€æµ‹ç½‘ç»œå…±äº«å·ç§¯ç‰¹å¾å›¾ï¼Œç›´æŽ¥åœ¨ç‰¹å¾å›¾ä¸Šç”Ÿæˆå€™é€‰æ¡†ï¼ˆanchor-basedï¼‰ï¼Œå®žçŽ°â€œç«¯åˆ°ç«¯â€è®­ç»ƒã€‚\n",
    "- **å¤šå°ºåº¦ anchor è®¾è®¡**ï¼šåœ¨æ¯ä¸ªä½ç½®é¢„è®¾å¤šä¸ªå°ºåº¦å’Œé•¿å®½æ¯”çš„ anchorï¼Œæœ‰æ•ˆåº”å¯¹ä¸åŒå¤§å°ç›®æ ‡ã€‚\n",
    "\n",
    "### 2. ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ\n",
    "\n",
    "- RPN å’Œ Fast R-CNN æ£€æµ‹å¤´å…±äº«å·ç§¯ç‰¹å¾ï¼Œé€šè¿‡äº¤æ›¿è®­ç»ƒæˆ–è¿‘ä¼¼è”åˆè®­ç»ƒï¼Œå®žçŽ°æ•´ä¸ªç½‘ç»œçš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚\n",
    "- è¿™æ˜¯é¦–æ¬¡å°†â€œåŒºåŸŸæè®® + ç›®æ ‡æ£€æµ‹â€ç»Ÿä¸€åˆ°ä¸€ä¸ªæ·±åº¦ç½‘ç»œä¸­ï¼Œæžå¤§æå‡æ•ˆçŽ‡å’Œä¸€è‡´æ€§ã€‚\n",
    "\n",
    "### 3. Anchor æœºåˆ¶çš„å¼€åˆ›æ€§åº”ç”¨\n",
    "\n",
    "- Faster R-CNN é¦–æ¬¡ç³»ç»ŸåŒ–ä½¿ç”¨ anchor boxes ä½œä¸ºå€™é€‰æ¡†åŸºç¡€ï¼Œè¿™ä¸€æ€æƒ³æ·±åˆ»å½±å“äº†åŽç»­å‡ ä¹Žæ‰€æœ‰ç›®æ ‡æ£€æµ‹å™¨ï¼ˆå¦‚ SSDã€YOLOv2+ã€RetinaNet ç­‰ï¼‰ã€‚\n",
    "- Anchor æœºåˆ¶è®©ç½‘ç»œå­¦ä¹ â€œç›¸å¯¹åç§»â€ï¼Œè€Œéžç»å¯¹åæ ‡ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "### 4. è®¡ç®—æ•ˆçŽ‡å¤§å¹…æå‡\n",
    "\n",
    "- ç›¸æ¯” R-CNNï¼ˆ~50s/å›¾ï¼‰å’Œ Fast R-CNNï¼ˆ~2s/å›¾ï¼‰ï¼ŒFaster R-CNN åœ¨ GPU ä¸Šå¯è¾¾ **5â€“10 FPS**ï¼ˆä½¿ç”¨ VGG16ï¼‰ï¼ŒçœŸæ­£è¿ˆå‘â€œè¿‘å®žæ—¶â€æ£€æµ‹ã€‚\n",
    "- å…±äº«å·ç§¯ç‰¹å¾é¿å…é‡å¤è®¡ç®—ï¼Œæ˜¯æ•ˆçŽ‡é£žè·ƒçš„å…³é”®ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ äºŒã€å½±å“åŠ›ä¸ŽåŽ†å²åœ°ä½\n",
    "\n",
    "### 1. å¥ å®š Two-Stage æ£€æµ‹å™¨çš„ä¸»æµæž¶æž„\n",
    "\n",
    "- æˆä¸ºåŽç»­å¦‚ **Mask R-CNN**ã€**Cascade R-CNN**ã€**Libra R-CNN** ç­‰é«˜æ€§èƒ½æ£€æµ‹å™¨çš„åŸºç¡€æ¡†æž¶ã€‚\n",
    "- â€œRPN + RoI Pooling + åˆ†ç±»/å›žå½’å¤´â€ æˆä¸ºæ ‡å‡†èŒƒå¼ã€‚\n",
    "\n",
    "### 2. æŽ¨åŠ¨ç›®æ ‡æ£€æµ‹è¿›å…¥æ·±åº¦å­¦ä¹ é»„é‡‘æ—¶ä»£\n",
    "\n",
    "- åœ¨ **PASCAL VOC**ã€**MS COCO** ç­‰æƒå¨æ•°æ®é›†ä¸Šé•¿æœŸéœ¸æ¦œï¼Œè¯æ˜Žæ·±åº¦å­¦ä¹ åœ¨æ£€æµ‹ä»»åŠ¡ä¸Šçš„å¼ºå¤§èƒ½åŠ›ã€‚\n",
    "- 2015 å¹´ COCO æ£€æµ‹å† å†›å³åŸºäºŽ Faster R-CNNï¼Œå¼•å‘å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¹¿æ³›å…³æ³¨ã€‚\n",
    "\n",
    "### 3. å·¥ä¸šè½åœ°å¹¿æ³›\n",
    "\n",
    "- è¢«å¹¿æ³›åº”ç”¨äºŽ **è‡ªåŠ¨é©¾é©¶**ï¼ˆå¦‚ Waymoï¼‰ã€**æœºå™¨äººè§†è§‰**ã€**å®‰é˜²ç›‘æŽ§**ã€**åŒ»ç–—å½±åƒ**ç­‰é¢†åŸŸã€‚\n",
    "- æˆä¸ºè®¸å¤šå•†ä¸šæ£€æµ‹ç³»ç»Ÿçš„åº•å±‚æž¶æž„ã€‚\n",
    "\n",
    "### 4. å¯å‘ One-Stage æ–¹æ³•çš„å‘å±•\n",
    "\n",
    "- å°½ç®¡æ˜¯ Two-Stage æ–¹æ³•ï¼Œä½†å…¶ **anchor æœºåˆ¶** è¢« SSDã€YOLOv2 ç­‰ One-Stage æ–¹æ³•å€Ÿé‰´ï¼ŒæŽ¨åŠ¨æ•´ä¸ªé¢†åŸŸæ¼”è¿›ã€‚\n",
    "- åŽç»­å¾ˆå¤šæ”¹è¿›ï¼ˆå¦‚ FPNã€ATSSã€IoU Lossï¼‰éƒ½å»ºç«‹åœ¨ Faster R-CNN æ¡†æž¶ä¹‹ä¸Šã€‚\n",
    "\n",
    "### 5. å­¦æœ¯å¼•ç”¨ä¸Žå¼€æºç”Ÿæ€\n",
    "\n",
    "- æˆªè‡³ 2024 å¹´ï¼Œè®ºæ–‡å¼•ç”¨é‡ **è¶… 4 ä¸‡æ¬¡**ï¼ˆGoogle Scholarï¼‰ï¼Œæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸè¢«å¼•æœ€é«˜çš„è®ºæ–‡ä¹‹ä¸€ã€‚\n",
    "- å®˜æ–¹ä»£ç å’Œä¼—å¤šå¤çŽ°ç‰ˆæœ¬ï¼ˆå¦‚ **Detectron**ã€**mmdetection**ï¼‰æžå¤§æŽ¨åŠ¨ç¤¾åŒºå‘å±•ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ€»ç»“ä¸€å¥è¯\n",
    "\n",
    "> **Faster R-CNN é€šè¿‡å¼•å…¥ RPN å’Œ anchor æœºåˆ¶ï¼Œé¦–æ¬¡å®žçŽ°ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„é«˜æ•ˆç›®æ ‡æ£€æµ‹æ¡†æž¶ï¼Œä¸ä»…å¤§å¹…æé€Ÿï¼Œæ›´å¥ å®šäº†çŽ°ä»£ç›®æ ‡æ£€æµ‹çš„åŸºç¡€æž¶æž„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹å‘å±•å²ä¸Šçš„åˆ†æ°´å²­ä¹‹ä½œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªåŠ¨é‡æ–°åŠ è½½å¤–éƒ¨moduleï¼Œä½¿å¾—ä¿®æ”¹ä»£ç ä¹‹åŽæ— éœ€é‡æ–°import\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hdd.device.utils import get_device\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒæ•°æ®çš„è·¯å¾„\n",
    "DATA_ROOT = Path(\"~/workspace/hands-dirty-on-dl/dataset\").expanduser()\n",
    "# è®¾ç½®é¢„è®­ç»ƒæ¨¡åž‹å‚æ•°è·¯å¾„\n",
    "TORCH_HUB_PATH = Path(\"~/workspace/hands-dirty-on-dl/pretrained_models\").expanduser()\n",
    "torch.hub.set_dir(TORCH_HUB_PATH)\n",
    "# æŒ‘é€‰æœ€åˆé€‚çš„è®­ç»ƒè®¾å¤‡\n",
    "DEVICE = get_device([\"cuda\", \"cpu\"])\n",
    "print(\"Use device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
    "from hdd.scripts.detection.coco_utils import _coco_remove_images_without_annotations\n",
    "\n",
    "COCO_ROOT = Path(DATA_ROOT).expanduser() / \"coco\"\n",
    "\n",
    "\n",
    "def get_coco(train: bool, transforms=None):\n",
    "    image_path = f\"{COCO_ROOT}/train2017\" if train else f\"{COCO_ROOT}/val2017\"\n",
    "    annotation_file_path = (\n",
    "        f\"{COCO_ROOT}/annotations/instances_train2017.json\"\n",
    "        if train\n",
    "        else f\"{COCO_ROOT}/annotations/instances_val2017.json\"\n",
    "    )\n",
    "\n",
    "    dataset = datasets.CocoDetection(\n",
    "        root=image_path,\n",
    "        annFile=annotation_file_path,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    target_keys = [\"boxes\", \"labels\", \"image_id\"]\n",
    "    dataset = wrap_dataset_for_transforms_v2(dataset, target_keys=target_keys)\n",
    "\n",
    "    if train:\n",
    "        dataset = _coco_remove_images_without_annotations(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        # v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ConvertBoundingBoxFormat(tv_tensors.BoundingBoxFormat.XYXY),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.ToPureTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ConvertBoundingBoxFormat(tv_tensors.BoundingBoxFormat.XYXY),\n",
    "        v2.ToPureTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = get_coco(train=True, transforms=train_transforms)\n",
    "val_dataset = get_coco(train=False, transforms=val_transforms)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "img, target = val_dataset[0]\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005373",
   "metadata": {},
   "source": [
    "### Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc60ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
    "\n",
    "# load a pre-trained model for classification and return only the features\n",
    "# mobilenet_backboneä¼šå°†BN Layerå†»ç»“ä¸”å¯æŒ‡å®šå¯è®­ç»ƒçš„å±‚æ•°\n",
    "backbone = mobilenet_backbone(\n",
    "    backbone_name=\"mobilenet_v2\", fpn=False, pretrained=True, trainable_layers=2\n",
    ")\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n",
    ")\n",
    "\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "num_classes = 91\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler,\n",
    ")\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0234ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.train.warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-3\n",
    "max_iterations = 50_000\n",
    "norm_params, other_params = torchvision.ops._utils.split_normalization_params(model)\n",
    "# Do not apply weight decay to norm parameters\n",
    "parameters = [\n",
    "    {\"params\": norm_params, \"weight_decay\": 0},\n",
    "    {\"params\": other_params, \"weight_decay\": weight_decay},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=learning_rate)\n",
    "\n",
    "base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, max_iterations, eta_min=1e-6\n",
    ")\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    multiplier=1.0,\n",
    "    total_epoch=2000,\n",
    "    after_scheduler=base_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.scripts.detection import utils\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, optimizer, scheduler, data_loader, device, epoch, print_freq, scaler=None\n",
    ") -> utils.MetricLogger:\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "        with torch.amp.autocast(enabled=scaler is not None, device_type=\"cuda\"):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75459ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from hdd.scripts.detection.coco_utils import get_coco_api_from_dataset\n",
    "from hdd.scripts.detection.coco_eval import CocoEvaluator\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b885666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "scaler = torch.amp.GradScaler()\n",
    "max_epochs = int(max_iterations / (len(train_dataset) / batch_size))\n",
    "print(f\"Max epochs: {max_epochs}\")\n",
    "for epoch in range(max_epochs):\n",
    "    train_one_epoch(\n",
    "        model, optimizer, scheduler, train_dataloader, DEVICE, epoch, 100, scaler\n",
    "    )\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88aba84",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "raw_img = decode_image(\"./resources/coco_test.jpg\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "img = val_transforms(raw_img, None)\n",
    "img = [img[0].to(DEVICE)]\n",
    "# Step 4: Use the model and visualize the predictions\n",
    "prediction = model(img)[0]\n",
    "\n",
    "# get labels for each prediction\n",
    "indices = torch.where(prediction[\"scores\"] > 0.5)\n",
    "labels = [\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT.meta[\"categories\"][i]\n",
    "    for i in prediction[\"labels\"][indices]\n",
    "]\n",
    "\n",
    "box = draw_bounding_boxes(\n",
    "    raw_img,\n",
    "    boxes=prediction[\"boxes\"][indices],\n",
    "    labels=labels,\n",
    "    colors=\"red\",\n",
    "    width=4,\n",
    "    font_size=30,\n",
    ")\n",
    "to_pil_image(box.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0603b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
