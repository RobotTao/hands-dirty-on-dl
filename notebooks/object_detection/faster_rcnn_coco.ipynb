{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3320239",
   "metadata": {},
   "source": [
    "# Faster R-CNN on Coco\n",
    "# 🚀 Faster R-CNN：目标检测的里程碑之作\n",
    "\n",
    "Faster R-CNN 是目标检测领域里程碑式的工作，由 Shaoqing Ren、Kaiming He、Ross Girshick 和 Jian Sun 在 2015 年提出（论文标题：[*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*](https://arxiv.org/abs/1506.01497)）。它在 R-CNN 系列中承前启后，解决了此前方法的速度瓶颈，同时保持了高精度。\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 一、核心创新点\n",
    "\n",
    "### 1. 引入 Region Proposal Network (RPN) —— 革命性的区域提议机制\n",
    "\n",
    "- **取代 Selective Search**：在 Fast R-CNN 中，区域提议仍依赖外部算法（如 Selective Search），速度慢且与网络分离。\n",
    "- **RPN 是全卷积网络**：与检测网络共享卷积特征图，直接在特征图上生成候选框（anchor-based），实现“端到端”训练。\n",
    "- **多尺度 anchor 设计**：在每个位置预设多个尺度和长宽比的 anchor，有效应对不同大小目标。\n",
    "\n",
    "### 2. 端到端联合训练\n",
    "\n",
    "- RPN 和 Fast R-CNN 检测头共享卷积特征，通过交替训练或近似联合训练，实现整个网络的端到端优化。\n",
    "- 这是首次将“区域提议 + 目标检测”统一到一个深度网络中，极大提升效率和一致性。\n",
    "\n",
    "### 3. Anchor 机制的开创性应用\n",
    "\n",
    "- Faster R-CNN 首次系统化使用 anchor boxes 作为候选框基础，这一思想深刻影响了后续几乎所有目标检测器（如 SSD、YOLOv2+、RetinaNet 等）。\n",
    "- Anchor 机制让网络学习“相对偏移”，而非绝对坐标，提升泛化能力。\n",
    "\n",
    "### 4. 计算效率大幅提升\n",
    "\n",
    "- 相比 R-CNN（~50s/图）和 Fast R-CNN（~2s/图），Faster R-CNN 在 GPU 上可达 **5–10 FPS**（使用 VGG16），真正迈向“近实时”检测。\n",
    "- 共享卷积特征避免重复计算，是效率飞跃的关键。\n",
    "\n",
    "---\n",
    "\n",
    "## 🌍 二、影响力与历史地位\n",
    "\n",
    "### 1. 奠定 Two-Stage 检测器的主流架构\n",
    "\n",
    "- 成为后续如 **Mask R-CNN**、**Cascade R-CNN**、**Libra R-CNN** 等高性能检测器的基础框架。\n",
    "- “RPN + RoI Pooling + 分类/回归头” 成为标准范式。\n",
    "\n",
    "### 2. 推动目标检测进入深度学习黄金时代\n",
    "\n",
    "- 在 **PASCAL VOC**、**MS COCO** 等权威数据集上长期霸榜，证明深度学习在检测任务上的强大能力。\n",
    "- 2015 年 COCO 检测冠军即基于 Faster R-CNN，引发学术界和工业界广泛关注。\n",
    "\n",
    "### 3. 工业落地广泛\n",
    "\n",
    "- 被广泛应用于 **自动驾驶**（如 Waymo）、**机器人视觉**、**安防监控**、**医疗影像**等领域。\n",
    "- 成为许多商业检测系统的底层架构。\n",
    "\n",
    "### 4. 启发 One-Stage 方法的发展\n",
    "\n",
    "- 尽管是 Two-Stage 方法，但其 **anchor 机制** 被 SSD、YOLOv2 等 One-Stage 方法借鉴，推动整个领域演进。\n",
    "- 后续很多改进（如 FPN、ATSS、IoU Loss）都建立在 Faster R-CNN 框架之上。\n",
    "\n",
    "### 5. 学术引用与开源生态\n",
    "\n",
    "- 截至 2024 年，论文引用量 **超 4 万次**（Google Scholar），是计算机视觉领域被引最高的论文之一。\n",
    "- 官方代码和众多复现版本（如 **Detectron**、**mmdetection**）极大推动社区发展。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结一句话\n",
    "\n",
    "> **Faster R-CNN 通过引入 RPN 和 anchor 机制，首次实现端到端可训练的高效目标检测框架，不仅大幅提速，更奠定了现代目标检测的基础架构，是深度学习目标检测发展史上的分水岭之作。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动重新加载外部module，使得修改代码之后无需重新import\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hdd.device.utils import get_device\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置训练数据的路径\n",
    "DATA_ROOT = Path(\"~/workspace/hands-dirty-on-dl/dataset\").expanduser()\n",
    "# 设置预训练模型参数路径\n",
    "TORCH_HUB_PATH = Path(\"~/workspace/hands-dirty-on-dl/pretrained_models\").expanduser()\n",
    "torch.hub.set_dir(TORCH_HUB_PATH)\n",
    "# 挑选最合适的训练设备\n",
    "DEVICE = get_device([\"cuda\", \"cpu\"])\n",
    "print(\"Use device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
    "from hdd.scripts.detection.coco_utils import _coco_remove_images_without_annotations\n",
    "\n",
    "COCO_ROOT = Path(DATA_ROOT).expanduser() / \"coco\"\n",
    "\n",
    "\n",
    "def get_coco(train: bool, transforms=None):\n",
    "    image_path = f\"{COCO_ROOT}/train2017\" if train else f\"{COCO_ROOT}/val2017\"\n",
    "    annotation_file_path = (\n",
    "        f\"{COCO_ROOT}/annotations/instances_train2017.json\"\n",
    "        if train\n",
    "        else f\"{COCO_ROOT}/annotations/instances_val2017.json\"\n",
    "    )\n",
    "\n",
    "    dataset = datasets.CocoDetection(\n",
    "        root=image_path,\n",
    "        annFile=annotation_file_path,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    target_keys = [\"boxes\", \"labels\", \"image_id\"]\n",
    "    dataset = wrap_dataset_for_transforms_v2(dataset, target_keys=target_keys)\n",
    "\n",
    "    if train:\n",
    "        dataset = _coco_remove_images_without_annotations(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        # v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ConvertBoundingBoxFormat(tv_tensors.BoundingBoxFormat.XYXY),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.ToPureTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ConvertBoundingBoxFormat(tv_tensors.BoundingBoxFormat.XYXY),\n",
    "        v2.ToPureTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = get_coco(train=True, transforms=train_transforms)\n",
    "val_dataset = get_coco(train=False, transforms=val_transforms)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "img, target = val_dataset[0]\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14005373",
   "metadata": {},
   "source": [
    "### Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc60ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
    "\n",
    "# load a pre-trained model for classification and return only the features\n",
    "# mobilenet_backbone会将BN Layer冻结且可指定可训练的层数\n",
    "backbone = mobilenet_backbone(\n",
    "    backbone_name=\"mobilenet_v2\", fpn=False, pretrained=True, trainable_layers=2\n",
    ")\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n",
    ")\n",
    "\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "num_classes = 91\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler,\n",
    ")\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0234ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.train.warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "weight_decay = 1e-5\n",
    "learning_rate = 1e-3\n",
    "max_iterations = 50_000\n",
    "norm_params, other_params = torchvision.ops._utils.split_normalization_params(model)\n",
    "# Do not apply weight decay to norm parameters\n",
    "parameters = [\n",
    "    {\"params\": norm_params, \"weight_decay\": 0},\n",
    "    {\"params\": other_params, \"weight_decay\": weight_decay},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=learning_rate)\n",
    "\n",
    "base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, max_iterations, eta_min=1e-6\n",
    ")\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    multiplier=1.0,\n",
    "    total_epoch=2000,\n",
    "    after_scheduler=base_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.scripts.detection import utils\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, optimizer, scheduler, data_loader, device, epoch, print_freq, scaler=None\n",
    ") -> utils.MetricLogger:\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "        with torch.amp.autocast(enabled=scaler is not None, device_type=\"cuda\"):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75459ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from hdd.scripts.detection.coco_utils import get_coco_api_from_dataset\n",
    "from hdd.scripts.detection.coco_eval import CocoEvaluator\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b885666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用混合精度训练\n",
    "scaler = torch.amp.GradScaler()\n",
    "max_epochs = int(max_iterations / (len(train_dataset) / batch_size))\n",
    "print(f\"Max epochs: {max_epochs}\")\n",
    "for epoch in range(max_epochs):\n",
    "    train_one_epoch(\n",
    "        model, optimizer, scheduler, train_dataloader, DEVICE, epoch, 100, scaler\n",
    "    )\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88aba84",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "raw_img = decode_image(\"./resources/coco_test.jpg\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "img = val_transforms(raw_img, None)\n",
    "img = [img[0].to(DEVICE)]\n",
    "# Step 4: Use the model and visualize the predictions\n",
    "prediction = model(img)[0]\n",
    "\n",
    "# get labels for each prediction\n",
    "indices = torch.where(prediction[\"scores\"] > 0.5)\n",
    "labels = [\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT.meta[\"categories\"][i]\n",
    "    for i in prediction[\"labels\"][indices]\n",
    "]\n",
    "\n",
    "box = draw_bounding_boxes(\n",
    "    raw_img,\n",
    "    boxes=prediction[\"boxes\"][indices],\n",
    "    labels=labels,\n",
    "    colors=\"red\",\n",
    "    width=4,\n",
    "    font_size=30,\n",
    ")\n",
    "to_pil_image(box.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0603b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
