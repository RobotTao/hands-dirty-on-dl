{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5525bf60",
   "metadata": {},
   "source": [
    "# MobileNet V2 关键创新点与缺点总结\n",
    "\n",
    "[MobileNet V2](https://arxiv.org/abs/1801.04381 ) 是 Google 提出的一种轻量级卷积神经网络架构，专为移动端和嵌入式设备设计。相比第一代 MobileNet，它在保持高效计算的同时显著提升了模型的准确性。\n",
    "\n",
    "---\n",
    "\n",
    "## 关键创新点\n",
    "\n",
    "### 1. **Inverted Residuals（倒残差结构）**\n",
    "\n",
    "- 使用“扩展-压缩”策略：\n",
    "  - 首先通过 `1x1` 卷积升维（扩展通道数）\n",
    "  - 然后使用 `3x3` 深度可分离卷积提取特征\n",
    "  - 最后通过 `1x1` 卷积降维（压缩通道数）\n",
    "- 该结构为两头小中间大,与传统残差模块中“压缩-扩展”的两头大中间小不同，因此称为“倒置”残差。\n",
    "- 这种设计可以在低维空间中保留更多信息，同时保持轻量化。\n",
    "\n",
    "### 2. **Linear Bottleneck（线性瓶颈层）**\n",
    "\n",
    "- 在最后的降维层中使用 **线性激活函数** 而不是 ReLU6。\n",
    "- 避免了非线性激活对特征表达能力的破坏，尤其在低维空间中尤为重要。\n",
    "\n",
    "这种设计基于如下观测: 低维流形通过线性变换升维之后在用ReLU这种非线性变换相较于降维损失更少的信息,这就是为何采用两头大中间小的结构,在使用ReLU6非线性激活函数时升维,且在降维时不采用ReLU6函数.\n",
    "\n",
    "  ![alt text](resources/mobilenetv2_manifold.png \"Title\")\n",
    "\n",
    "  ![alt text](resources/mobilenetv2_inverted_residual_block.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d1e83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70544e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动重新加载外部module，使得修改代码之后无需重新import\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hdd.device.utils import get_device\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 设置训练数据的路径\n",
    "DATA_ROOT = \"~/workspace/hands-dirty-on-dl/dataset\"\n",
    "# 设置TensorBoard的路径\n",
    "TENSORBOARD_ROOT = \"~/workspace/hands-dirty-on-dl/dataset\"\n",
    "# 设置预训练模型参数路径\n",
    "TORCH_HUB_PATH = \"~/workspace/hands-dirty-on-dl/pretrained_models\"\n",
    "torch.hub.set_dir(TORCH_HUB_PATH)\n",
    "# 挑选最合适的训练设备\n",
    "DEVICE = get_device([\"cuda\", \"cpu\"])\n",
    "print(\"Use device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.dataset.imagenette_in_memory import ImagenetteInMemory\n",
    "from hdd.data_util.transforms import RandomResize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "TRAIN_MEAN = [0.4625, 0.4580, 0.4295]\n",
    "TRAIN_STD = [0.2452, 0.2390, 0.2469]\n",
    "train_dataset_transforms = transforms.Compose(\n",
    "    [\n",
    "        RandomResize([256, 296, 384]),  # 随机在三个size中选择一个进行resize\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),\n",
    "    ]\n",
    ")\n",
    "val_dataset_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),\n",
    "    ]\n",
    ")\n",
    "train_dataset = ImagenetteInMemory(\n",
    "    root=DATA_ROOT,\n",
    "    split=\"train\",\n",
    "    size=\"full\",\n",
    "    download=True,\n",
    "    transform=train_dataset_transforms,\n",
    ")\n",
    "val_dataset = ImagenetteInMemory(\n",
    "    root=DATA_ROOT,\n",
    "    split=\"val\",\n",
    "    size=\"full\",\n",
    "    download=True,\n",
    "    transform=val_dataset_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataloader(batch_size, train_dataset, val_dataset):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.models.cnn.mobilenet_v2 import MobileNetV2\n",
    "from hdd.train.classification_utils import (\n",
    "    naive_train_classification_model,\n",
    "    eval_image_classifier,\n",
    ")\n",
    "from hdd.models.nn_utils import count_trainable_parameter\n",
    "\n",
    "\n",
    "def train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    max_epochs=100,\n",
    ") -> tuple[MobileNetV2, dict[str, list[float]]]:\n",
    "    net = MobileNetV2(num_classes=10, width_multiplier=width_multiplier).to(DEVICE)\n",
    "    print(f\"#Parameter: {count_trainable_parameter(net)}\")\n",
    "    criteria = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, max_epochs, eta_min=lr / 100\n",
    "    )\n",
    "    training_stats = naive_train_classification_model(\n",
    "        net,\n",
    "        criteria,\n",
    "        max_epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        DEVICE,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return net, training_stats\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = build_dataloader(64, train_dataset, val_dataset)\n",
    "\n",
    "net, width_multiplier_1 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=1,\n",
    "    lr=0.001,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f41a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, width_multiplier_75 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.75,\n",
    "    lr=0.001,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, width_multiplier_50 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.5,\n",
    "    lr=0.001,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, width_multiplier_25 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.25,\n",
    "    lr=0.001,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,12))\n",
    "fields = width_multiplier_1.keys()\n",
    "for i, field in enumerate(fields):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot(width_multiplier_1[field], label=\"Width-1\", linestyle=\"--\")\n",
    "    plt.plot(width_multiplier_75[field], label=\"Width-0.75\")\n",
    "    plt.plot(width_multiplier_50[field], label=\"Width-0.5\", linestyle=\"--\")\n",
    "    plt.plot(width_multiplier_25[field], label=\"Width-0.25\", linestyle=\"-\")\n",
    "    plt.legend()\n",
    "    plt.title(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2f2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
