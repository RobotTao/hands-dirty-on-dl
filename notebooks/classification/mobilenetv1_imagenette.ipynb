{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5525bf60",
   "metadata": {},
   "source": [
    "# MobileNet V1 模型总结\n",
    "\n",
    "## 关键创新点\n",
    "\n",
    "### 1. 深度可分离卷积（Depthwise Separable Convolution）\n",
    "- 将标准卷积分解为两个独立操作：\n",
    "  - **Depthwise Convolution（逐通道卷积）**：对每个输入通道单独进行卷积操作。\n",
    "  - **Pointwise Convolution（逐点卷积）**：使用 `1x1` 卷积融合不同通道的信息。\n",
    "- 大幅减少了计算量和参数数量，适用于移动端和嵌入式设备。这一点和新出的MLP-Mixer的结构及其接近.\n",
    "\n",
    "### 2. 轻量化设计\n",
    "- 在保证准确率的前提下，通过减少冗余计算实现高效推理。\n",
    "- 相比传统卷积网络（如VGG、Inception等），模型大小显著减小。\n",
    "\n",
    "### 3. 宽度乘子（Width Multiplier）\n",
    "- 引入超参数 `α ∈ (0,1]` 控制输入输出通道数，进一步压缩模型。\n",
    "- 可以在精度与速度之间做权衡，提升部署灵活性。\n",
    "\n",
    "### 4. 分辨率乘子（Resolution Multiplier）\n",
    "- 控制输入图像的分辨率，作为另一个控制模型复杂度的参数。\n",
    "- 允许根据设备性能调整输入尺寸，从而影响整体计算量。\n",
    "\n",
    "### 5. 模块化结构\n",
    "- 整体网络由多个堆叠的深度可分离卷积模块构成，便于复用和扩展。\n",
    "\n",
    "---\n",
    "\n",
    "## 缺点与局限性\n",
    "\n",
    "### 1. 精度略低于标准模型\n",
    "- 在相同数据集下，相比 ResNet、Inception 等大型模型，在 Top-1 准确率上略有下降。\n",
    "\n",
    "### 2. 感受野受限\n",
    "- 使用较多的小卷积核（如 3x3）和深度卷积，可能限制了特征提取的感受野范围。\n",
    "\n",
    "### 3. 依赖手动设计\n",
    "- 网络结构是人工设计的，没有像后续版本（如 MobileNetV2、NASNet）那样利用神经网络架构搜索（NAS）来优化性能。\n",
    "\n",
    "### 4. 信息流动效率较低\n",
    "- 深度可分离卷积可能导致特征表达能力受限，特别是在高层语义任务中表现不如密集连接的网络。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "MobileNet V1 是一个开创性的轻量级卷积神经网络，其核心思想 —— **深度可分离卷积**，为后续轻量化模型的发展奠定了基础。尽管它在精度和表达能力上有所妥协，但在移动设备和边缘计算场景中具有重要的应用价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d1e83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70544e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# 自动重新加载外部module，使得修改代码之后无需重新import\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hdd.device.utils import get_device\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 设置训练数据的路径\n",
    "DATA_ROOT = \"~/workspace/hands-dirty-on-dl/dataset\"\n",
    "# 设置TensorBoard的路径\n",
    "TENSORBOARD_ROOT = \"~/workspace/hands-dirty-on-dl/dataset\"\n",
    "# 设置预训练模型参数路径\n",
    "TORCH_HUB_PATH = \"~/workspace/hands-dirty-on-dl/pretrained_models\"\n",
    "torch.hub.set_dir(TORCH_HUB_PATH)\n",
    "# 挑选最合适的训练设备\n",
    "DEVICE = get_device([\"cuda\", \"cpu\"])\n",
    "print(\"Use device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1e1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdd.dataset.imagenette_in_memory import ImagenetteInMemory\n",
    "from hdd.data_util.transforms import RandomResize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "TRAIN_MEAN = [0.4625, 0.4580, 0.4295]\n",
    "TRAIN_STD = [0.2452, 0.2390, 0.2469]\n",
    "train_dataset_transforms = transforms.Compose(\n",
    "    [\n",
    "        RandomResize([256, 296, 384]),  # 随机在三个size中选择一个进行resize\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),\n",
    "    ]\n",
    ")\n",
    "val_dataset_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),\n",
    "    ]\n",
    ")\n",
    "train_dataset = ImagenetteInMemory(\n",
    "    root=DATA_ROOT,\n",
    "    split=\"train\",\n",
    "    size=\"full\",\n",
    "    download=True,\n",
    "    transform=train_dataset_transforms,\n",
    ")\n",
    "val_dataset = ImagenetteInMemory(\n",
    "    root=DATA_ROOT,\n",
    "    split=\"val\",\n",
    "    size=\"full\",\n",
    "    download=True,\n",
    "    transform=val_dataset_transforms,\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataloader(batch_size, train_dataset, val_dataset):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Parameter: 3228170\n",
      "Epoch: 1/150 Train Loss: 2.1544 Accuracy: 0.1938 Time: 7.30377  | Val Loss: 2.6199 Accuracy: 0.1995\n",
      "Epoch: 2/150 Train Loss: 1.8218 Accuracy: 0.3549 Time: 7.14844  | Val Loss: 2.4292 Accuracy: 0.3078\n",
      "Epoch: 3/150 Train Loss: 1.5882 Accuracy: 0.4516 Time: 7.38312  | Val Loss: 1.9080 Accuracy: 0.3748\n",
      "Epoch: 4/150 Train Loss: 1.4126 Accuracy: 0.5257 Time: 7.18147  | Val Loss: 1.5030 Accuracy: 0.5200\n",
      "Epoch: 5/150 Train Loss: 1.3219 Accuracy: 0.5570 Time: 7.21082  | Val Loss: 1.3730 Accuracy: 0.5457\n",
      "Epoch: 6/150 Train Loss: 1.2387 Accuracy: 0.5925 Time: 7.25243  | Val Loss: 1.1495 Accuracy: 0.6227\n",
      "Epoch: 7/150 Train Loss: 1.1511 Accuracy: 0.6235 Time: 7.22659  | Val Loss: 1.1468 Accuracy: 0.6252\n",
      "Epoch: 8/150 Train Loss: 1.0970 Accuracy: 0.6419 Time: 7.17840  | Val Loss: 1.0378 Accuracy: 0.6650\n",
      "Epoch: 9/150 Train Loss: 1.0533 Accuracy: 0.6596 Time: 7.22944  | Val Loss: 0.9100 Accuracy: 0.6968\n",
      "Epoch: 10/150 Train Loss: 1.0110 Accuracy: 0.6720 Time: 7.20997  | Val Loss: 0.8965 Accuracy: 0.7080\n",
      "Epoch: 11/150 Train Loss: 0.9493 Accuracy: 0.6930 Time: 6.92071  | Val Loss: 0.9239 Accuracy: 0.7034\n",
      "Epoch: 12/150 Train Loss: 0.9099 Accuracy: 0.7037 Time: 6.98701  | Val Loss: 0.9118 Accuracy: 0.7052\n",
      "Epoch: 13/150 Train Loss: 0.8740 Accuracy: 0.7181 Time: 6.99973  | Val Loss: 0.8085 Accuracy: 0.7419\n",
      "Epoch: 14/150 Train Loss: 0.8394 Accuracy: 0.7218 Time: 7.08214  | Val Loss: 0.8755 Accuracy: 0.7208\n",
      "Epoch: 15/150 Train Loss: 0.8139 Accuracy: 0.7333 Time: 6.99927  | Val Loss: 0.7611 Accuracy: 0.7592\n",
      "Epoch: 16/150 Train Loss: 0.7870 Accuracy: 0.7433 Time: 7.13728  | Val Loss: 0.8397 Accuracy: 0.7310\n",
      "Epoch: 17/150 Train Loss: 0.7745 Accuracy: 0.7468 Time: 7.19253  | Val Loss: 0.7523 Accuracy: 0.7552\n",
      "Epoch: 18/150 Train Loss: 0.7520 Accuracy: 0.7570 Time: 7.15605  | Val Loss: 0.7413 Accuracy: 0.7603\n",
      "Epoch: 19/150 Train Loss: 0.7437 Accuracy: 0.7592 Time: 7.09854  | Val Loss: 0.7007 Accuracy: 0.7794\n",
      "Epoch: 20/150 Train Loss: 0.6963 Accuracy: 0.7759 Time: 7.07962  | Val Loss: 0.7387 Accuracy: 0.7682\n",
      "Epoch: 21/150 Train Loss: 0.6857 Accuracy: 0.7785 Time: 7.09960  | Val Loss: 0.7046 Accuracy: 0.7789\n",
      "Epoch: 22/150 Train Loss: 0.6736 Accuracy: 0.7788 Time: 7.06825  | Val Loss: 0.8541 Accuracy: 0.7366\n",
      "Epoch: 23/150 Train Loss: 0.6498 Accuracy: 0.7847 Time: 6.91468  | Val Loss: 0.7634 Accuracy: 0.7692\n",
      "Epoch: 24/150 Train Loss: 0.6412 Accuracy: 0.7935 Time: 6.91173  | Val Loss: 0.7033 Accuracy: 0.7837\n",
      "Epoch: 25/150 Train Loss: 0.6307 Accuracy: 0.8012 Time: 7.15195  | Val Loss: 0.6821 Accuracy: 0.7883\n",
      "Epoch: 26/150 Train Loss: 0.6107 Accuracy: 0.8030 Time: 7.19124  | Val Loss: 0.6575 Accuracy: 0.7954\n",
      "Epoch: 27/150 Train Loss: 0.5913 Accuracy: 0.8079 Time: 7.03539  | Val Loss: 0.6490 Accuracy: 0.7972\n",
      "Epoch: 28/150 Train Loss: 0.5749 Accuracy: 0.8102 Time: 7.10475  | Val Loss: 0.6186 Accuracy: 0.7987\n",
      "Epoch: 29/150 Train Loss: 0.5617 Accuracy: 0.8167 Time: 7.21325  | Val Loss: 0.5892 Accuracy: 0.8201\n",
      "Epoch: 30/150 Train Loss: 0.5430 Accuracy: 0.8225 Time: 7.10055  | Val Loss: 0.6483 Accuracy: 0.7896\n",
      "Epoch: 31/150 Train Loss: 0.5336 Accuracy: 0.8231 Time: 7.03631  | Val Loss: 0.6078 Accuracy: 0.8158\n",
      "Epoch: 32/150 Train Loss: 0.5273 Accuracy: 0.8285 Time: 7.10976  | Val Loss: 0.6181 Accuracy: 0.8089\n",
      "Epoch: 33/150 Train Loss: 0.5030 Accuracy: 0.8400 Time: 7.09969  | Val Loss: 0.6734 Accuracy: 0.8013\n",
      "Epoch: 34/150 Train Loss: 0.4978 Accuracy: 0.8360 Time: 7.11859  | Val Loss: 0.5661 Accuracy: 0.8268\n",
      "Epoch: 35/150 Train Loss: 0.4912 Accuracy: 0.8418 Time: 7.02655  | Val Loss: 0.6027 Accuracy: 0.8163\n",
      "Epoch: 36/150 Train Loss: 0.4975 Accuracy: 0.8362 Time: 7.01340  | Val Loss: 0.6929 Accuracy: 0.7918\n",
      "Epoch: 37/150 Train Loss: 0.4804 Accuracy: 0.8399 Time: 7.18351  | Val Loss: 0.6305 Accuracy: 0.8074\n",
      "Epoch: 38/150 Train Loss: 0.4470 Accuracy: 0.8547 Time: 7.12569  | Val Loss: 0.7325 Accuracy: 0.7890\n",
      "Epoch: 39/150 Train Loss: 0.4406 Accuracy: 0.8541 Time: 7.10315  | Val Loss: 0.5945 Accuracy: 0.8268\n",
      "Epoch: 40/150 Train Loss: 0.4441 Accuracy: 0.8524 Time: 7.09051  | Val Loss: 0.6017 Accuracy: 0.8201\n",
      "Epoch: 41/150 Train Loss: 0.4129 Accuracy: 0.8641 Time: 7.10828  | Val Loss: 0.5851 Accuracy: 0.8232\n",
      "Epoch: 42/150 Train Loss: 0.4108 Accuracy: 0.8636 Time: 7.24493  | Val Loss: 0.5696 Accuracy: 0.8326\n",
      "Epoch: 43/150 Train Loss: 0.4185 Accuracy: 0.8625 Time: 7.02882  | Val Loss: 0.5814 Accuracy: 0.8293\n",
      "Epoch: 44/150 Train Loss: 0.4161 Accuracy: 0.8634 Time: 7.04355  | Val Loss: 0.5273 Accuracy: 0.8420\n",
      "Epoch: 45/150 Train Loss: 0.3928 Accuracy: 0.8714 Time: 7.03743  | Val Loss: 0.6666 Accuracy: 0.8120\n",
      "Epoch: 46/150 Train Loss: 0.3753 Accuracy: 0.8791 Time: 7.03821  | Val Loss: 0.5640 Accuracy: 0.8324\n",
      "Epoch: 47/150 Train Loss: 0.3962 Accuracy: 0.8735 Time: 7.17686  | Val Loss: 0.6409 Accuracy: 0.8130\n",
      "Epoch: 48/150 Train Loss: 0.3774 Accuracy: 0.8777 Time: 7.06055  | Val Loss: 0.5591 Accuracy: 0.8336\n",
      "Epoch: 49/150 Train Loss: 0.3686 Accuracy: 0.8809 Time: 7.14983  | Val Loss: 0.5722 Accuracy: 0.8308\n",
      "Epoch: 50/150 Train Loss: 0.3650 Accuracy: 0.8800 Time: 7.06637  | Val Loss: 0.5298 Accuracy: 0.8380\n",
      "Epoch: 51/150 Train Loss: 0.3516 Accuracy: 0.8825 Time: 7.18391  | Val Loss: 0.5366 Accuracy: 0.8456\n",
      "Epoch: 52/150 Train Loss: 0.3520 Accuracy: 0.8840 Time: 7.04894  | Val Loss: 0.5718 Accuracy: 0.8324\n",
      "Epoch: 53/150 Train Loss: 0.3401 Accuracy: 0.8869 Time: 7.23052  | Val Loss: 0.5948 Accuracy: 0.8237\n",
      "Epoch: 54/150 Train Loss: 0.3427 Accuracy: 0.8896 Time: 7.20716  | Val Loss: 0.5427 Accuracy: 0.8423\n",
      "Epoch: 55/150 Train Loss: 0.3320 Accuracy: 0.8927 Time: 7.33890  | Val Loss: 0.5384 Accuracy: 0.8456\n",
      "Epoch: 56/150 Train Loss: 0.3179 Accuracy: 0.8969 Time: 7.16390  | Val Loss: 0.5617 Accuracy: 0.8418\n",
      "Epoch: 57/150 Train Loss: 0.3186 Accuracy: 0.8980 Time: 7.14960  | Val Loss: 0.4991 Accuracy: 0.8525\n",
      "Epoch: 58/150 Train Loss: 0.3048 Accuracy: 0.9022 Time: 7.15640  | Val Loss: 0.5825 Accuracy: 0.8313\n",
      "Epoch: 59/150 Train Loss: 0.3208 Accuracy: 0.8952 Time: 7.11366  | Val Loss: 0.5460 Accuracy: 0.8471\n",
      "Epoch: 60/150 Train Loss: 0.3061 Accuracy: 0.8984 Time: 7.20385  | Val Loss: 0.5442 Accuracy: 0.8456\n",
      "Epoch: 61/150 Train Loss: 0.2793 Accuracy: 0.9096 Time: 7.21263  | Val Loss: 0.5452 Accuracy: 0.8438\n",
      "Epoch: 62/150 Train Loss: 0.2980 Accuracy: 0.9032 Time: 7.09396  | Val Loss: 0.5304 Accuracy: 0.8482\n",
      "Epoch: 63/150 Train Loss: 0.2773 Accuracy: 0.9107 Time: 7.27484  | Val Loss: 0.5345 Accuracy: 0.8568\n",
      "Epoch: 64/150 Train Loss: 0.2689 Accuracy: 0.9116 Time: 7.20761  | Val Loss: 0.5115 Accuracy: 0.8487\n",
      "Epoch: 65/150 Train Loss: 0.2687 Accuracy: 0.9096 Time: 7.27134  | Val Loss: 0.5521 Accuracy: 0.8375\n",
      "Epoch: 66/150 Train Loss: 0.2622 Accuracy: 0.9155 Time: 7.26348  | Val Loss: 0.5382 Accuracy: 0.8489\n",
      "Epoch: 67/150 Train Loss: 0.2573 Accuracy: 0.9150 Time: 7.22023  | Val Loss: 0.5427 Accuracy: 0.8492\n",
      "Epoch: 68/150 Train Loss: 0.2594 Accuracy: 0.9159 Time: 7.23248  | Val Loss: 0.5794 Accuracy: 0.8369\n",
      "Epoch: 69/150 Train Loss: 0.2563 Accuracy: 0.9174 Time: 7.21338  | Val Loss: 0.5457 Accuracy: 0.8494\n",
      "Epoch: 70/150 Train Loss: 0.2493 Accuracy: 0.9190 Time: 7.25192  | Val Loss: 0.5338 Accuracy: 0.8482\n",
      "Epoch: 71/150 Train Loss: 0.2377 Accuracy: 0.9208 Time: 7.25937  | Val Loss: 0.5594 Accuracy: 0.8436\n",
      "Epoch: 72/150 Train Loss: 0.2399 Accuracy: 0.9223 Time: 7.25827  | Val Loss: 0.5583 Accuracy: 0.8487\n",
      "Epoch: 73/150 Train Loss: 0.2405 Accuracy: 0.9210 Time: 7.33346  | Val Loss: 0.5826 Accuracy: 0.8408\n",
      "Epoch: 74/150 Train Loss: 0.2341 Accuracy: 0.9228 Time: 7.21385  | Val Loss: 0.5392 Accuracy: 0.8520\n",
      "Epoch: 75/150 Train Loss: 0.2220 Accuracy: 0.9281 Time: 7.22681  | Val Loss: 0.5301 Accuracy: 0.8548\n",
      "Epoch: 76/150 Train Loss: 0.2229 Accuracy: 0.9253 Time: 7.19988  | Val Loss: 0.5007 Accuracy: 0.8583\n",
      "Epoch: 77/150 Train Loss: 0.2284 Accuracy: 0.9221 Time: 7.27869  | Val Loss: 0.4938 Accuracy: 0.8594\n",
      "Epoch: 78/150 Train Loss: 0.2176 Accuracy: 0.9280 Time: 7.17811  | Val Loss: 0.4969 Accuracy: 0.8673\n",
      "Epoch: 79/150 Train Loss: 0.2037 Accuracy: 0.9356 Time: 7.18479  | Val Loss: 0.5067 Accuracy: 0.8594\n",
      "Epoch: 80/150 Train Loss: 0.1939 Accuracy: 0.9351 Time: 7.38432  | Val Loss: 0.4882 Accuracy: 0.8624\n",
      "Epoch: 81/150 Train Loss: 0.1996 Accuracy: 0.9359 Time: 7.18169  | Val Loss: 0.5278 Accuracy: 0.8591\n",
      "Epoch: 82/150 Train Loss: 0.1959 Accuracy: 0.9373 Time: 7.21290  | Val Loss: 0.5196 Accuracy: 0.8604\n",
      "Epoch: 83/150 Train Loss: 0.1944 Accuracy: 0.9357 Time: 7.10719  | Val Loss: 0.4977 Accuracy: 0.8609\n",
      "Epoch: 84/150 Train Loss: 0.1950 Accuracy: 0.9366 Time: 7.13852  | Val Loss: 0.5036 Accuracy: 0.8601\n",
      "Epoch: 85/150 Train Loss: 0.1951 Accuracy: 0.9341 Time: 7.17728  | Val Loss: 0.4999 Accuracy: 0.8614\n",
      "Epoch: 86/150 Train Loss: 0.1845 Accuracy: 0.9411 Time: 7.15321  | Val Loss: 0.5082 Accuracy: 0.8629\n",
      "Epoch: 87/150 Train Loss: 0.1898 Accuracy: 0.9373 Time: 7.09049  | Val Loss: 0.5144 Accuracy: 0.8581\n",
      "Epoch: 88/150 Train Loss: 0.1869 Accuracy: 0.9416 Time: 7.17830  | Val Loss: 0.5331 Accuracy: 0.8561\n",
      "Epoch: 89/150 Train Loss: 0.1728 Accuracy: 0.9469 Time: 7.24080  | Val Loss: 0.4987 Accuracy: 0.8637\n",
      "Epoch: 90/150 Train Loss: 0.1875 Accuracy: 0.9401 Time: 7.23564  | Val Loss: 0.5120 Accuracy: 0.8637\n",
      "Epoch: 91/150 Train Loss: 0.1676 Accuracy: 0.9448 Time: 7.25164  | Val Loss: 0.5163 Accuracy: 0.8596\n",
      "Epoch: 92/150 Train Loss: 0.1684 Accuracy: 0.9467 Time: 7.22190  | Val Loss: 0.4987 Accuracy: 0.8660\n",
      "Epoch: 93/150 Train Loss: 0.1648 Accuracy: 0.9471 Time: 7.28297  | Val Loss: 0.5505 Accuracy: 0.8573\n",
      "Epoch: 94/150 Train Loss: 0.1543 Accuracy: 0.9506 Time: 7.14832  | Val Loss: 0.5131 Accuracy: 0.8650\n",
      "Epoch: 95/150 Train Loss: 0.1632 Accuracy: 0.9478 Time: 7.15692  | Val Loss: 0.5228 Accuracy: 0.8609\n",
      "Epoch: 96/150 Train Loss: 0.1676 Accuracy: 0.9443 Time: 7.20898  | Val Loss: 0.5146 Accuracy: 0.8642\n",
      "Epoch: 97/150 Train Loss: 0.1635 Accuracy: 0.9475 Time: 7.21082  | Val Loss: 0.4922 Accuracy: 0.8680\n",
      "Epoch: 98/150 Train Loss: 0.1442 Accuracy: 0.9547 Time: 7.12242  | Val Loss: 0.5153 Accuracy: 0.8634\n",
      "Epoch: 99/150 Train Loss: 0.1469 Accuracy: 0.9545 Time: 7.17742  | Val Loss: 0.5059 Accuracy: 0.8650\n",
      "Epoch: 100/150 Train Loss: 0.1419 Accuracy: 0.9541 Time: 7.15639  | Val Loss: 0.5100 Accuracy: 0.8655\n",
      "Epoch: 101/150 Train Loss: 0.1466 Accuracy: 0.9521 Time: 7.17486  | Val Loss: 0.5222 Accuracy: 0.8624\n",
      "Epoch: 102/150 Train Loss: 0.1399 Accuracy: 0.9562 Time: 7.16167  | Val Loss: 0.5117 Accuracy: 0.8614\n",
      "Epoch: 103/150 Train Loss: 0.1383 Accuracy: 0.9566 Time: 7.16963  | Val Loss: 0.5078 Accuracy: 0.8652\n",
      "Epoch: 104/150 Train Loss: 0.1380 Accuracy: 0.9565 Time: 7.18514  | Val Loss: 0.5160 Accuracy: 0.8624\n",
      "Epoch: 105/150 Train Loss: 0.1470 Accuracy: 0.9522 Time: 7.14621  | Val Loss: 0.5102 Accuracy: 0.8627\n",
      "Epoch: 106/150 Train Loss: 0.1314 Accuracy: 0.9581 Time: 7.18023  | Val Loss: 0.4958 Accuracy: 0.8665\n",
      "Epoch: 107/150 Train Loss: 0.1316 Accuracy: 0.9573 Time: 7.13926  | Val Loss: 0.5172 Accuracy: 0.8609\n",
      "Epoch: 108/150 Train Loss: 0.1359 Accuracy: 0.9548 Time: 7.16619  | Val Loss: 0.5169 Accuracy: 0.8622\n",
      "Epoch: 109/150 Train Loss: 0.1255 Accuracy: 0.9606 Time: 7.13556  | Val Loss: 0.5007 Accuracy: 0.8665\n",
      "Epoch: 110/150 Train Loss: 0.1357 Accuracy: 0.9579 Time: 7.18886  | Val Loss: 0.5155 Accuracy: 0.8660\n",
      "Epoch: 111/150 Train Loss: 0.1285 Accuracy: 0.9591 Time: 7.14879  | Val Loss: 0.5064 Accuracy: 0.8642\n",
      "Epoch: 112/150 Train Loss: 0.1305 Accuracy: 0.9598 Time: 7.16914  | Val Loss: 0.5180 Accuracy: 0.8657\n",
      "Epoch: 113/150 Train Loss: 0.1235 Accuracy: 0.9601 Time: 7.14062  | Val Loss: 0.5078 Accuracy: 0.8647\n",
      "Epoch: 114/150 Train Loss: 0.1258 Accuracy: 0.9601 Time: 7.18797  | Val Loss: 0.5030 Accuracy: 0.8617\n",
      "Epoch: 115/150 Train Loss: 0.1137 Accuracy: 0.9638 Time: 7.17575  | Val Loss: 0.4928 Accuracy: 0.8632\n",
      "Epoch: 116/150 Train Loss: 0.1226 Accuracy: 0.9609 Time: 7.15183  | Val Loss: 0.5222 Accuracy: 0.8657\n",
      "Epoch: 117/150 Train Loss: 0.1120 Accuracy: 0.9627 Time: 7.24070  | Val Loss: 0.5097 Accuracy: 0.8690\n",
      "Epoch: 118/150 Train Loss: 0.1205 Accuracy: 0.9630 Time: 7.20099  | Val Loss: 0.5014 Accuracy: 0.8639\n",
      "Epoch: 119/150 Train Loss: 0.1208 Accuracy: 0.9625 Time: 7.18375  | Val Loss: 0.5032 Accuracy: 0.8696\n",
      "Epoch: 120/150 Train Loss: 0.1193 Accuracy: 0.9606 Time: 7.31259  | Val Loss: 0.5010 Accuracy: 0.8690\n",
      "Epoch: 121/150 Train Loss: 0.1160 Accuracy: 0.9629 Time: 7.17647  | Val Loss: 0.4987 Accuracy: 0.8650\n",
      "Epoch: 122/150 Train Loss: 0.1139 Accuracy: 0.9649 Time: 7.23005  | Val Loss: 0.4965 Accuracy: 0.8675\n",
      "Epoch: 123/150 Train Loss: 0.1126 Accuracy: 0.9664 Time: 7.22248  | Val Loss: 0.4964 Accuracy: 0.8668\n",
      "Epoch: 124/150 Train Loss: 0.1037 Accuracy: 0.9682 Time: 7.18941  | Val Loss: 0.5003 Accuracy: 0.8668\n",
      "Epoch: 125/150 Train Loss: 0.1058 Accuracy: 0.9672 Time: 7.18142  | Val Loss: 0.4853 Accuracy: 0.8668\n",
      "Epoch: 126/150 Train Loss: 0.1068 Accuracy: 0.9672 Time: 7.19089  | Val Loss: 0.4914 Accuracy: 0.8683\n",
      "Epoch: 127/150 Train Loss: 0.1006 Accuracy: 0.9678 Time: 7.19571  | Val Loss: 0.4979 Accuracy: 0.8673\n",
      "Epoch: 128/150 Train Loss: 0.1074 Accuracy: 0.9650 Time: 7.19729  | Val Loss: 0.5051 Accuracy: 0.8647\n",
      "Epoch: 129/150 Train Loss: 0.1017 Accuracy: 0.9677 Time: 7.18019  | Val Loss: 0.5016 Accuracy: 0.8673\n",
      "Epoch: 130/150 Train Loss: 0.1062 Accuracy: 0.9680 Time: 7.20145  | Val Loss: 0.5025 Accuracy: 0.8660\n",
      "Epoch: 131/150 Train Loss: 0.1052 Accuracy: 0.9669 Time: 7.19926  | Val Loss: 0.4947 Accuracy: 0.8662\n",
      "Epoch: 132/150 Train Loss: 0.1000 Accuracy: 0.9684 Time: 7.17292  | Val Loss: 0.5007 Accuracy: 0.8675\n",
      "Epoch: 133/150 Train Loss: 0.1015 Accuracy: 0.9673 Time: 7.21737  | Val Loss: 0.5045 Accuracy: 0.8660\n",
      "Epoch: 134/150 Train Loss: 0.1028 Accuracy: 0.9675 Time: 7.20621  | Val Loss: 0.5029 Accuracy: 0.8683\n",
      "Epoch: 135/150 Train Loss: 0.0969 Accuracy: 0.9706 Time: 7.16307  | Val Loss: 0.4958 Accuracy: 0.8683\n",
      "Epoch: 136/150 Train Loss: 0.1007 Accuracy: 0.9682 Time: 7.12315  | Val Loss: 0.4948 Accuracy: 0.8706\n",
      "Epoch: 137/150 Train Loss: 0.1039 Accuracy: 0.9682 Time: 7.16417  | Val Loss: 0.4901 Accuracy: 0.8696\n",
      "Epoch: 138/150 Train Loss: 0.1002 Accuracy: 0.9690 Time: 7.17677  | Val Loss: 0.4906 Accuracy: 0.8708\n",
      "Epoch: 139/150 Train Loss: 0.1028 Accuracy: 0.9691 Time: 7.20617  | Val Loss: 0.5018 Accuracy: 0.8690\n",
      "Epoch: 140/150 Train Loss: 0.1062 Accuracy: 0.9653 Time: 7.14743  | Val Loss: 0.4962 Accuracy: 0.8683\n",
      "Epoch: 141/150 Train Loss: 0.1056 Accuracy: 0.9678 Time: 7.13232  | Val Loss: 0.4907 Accuracy: 0.8703\n",
      "Epoch: 142/150 Train Loss: 0.0936 Accuracy: 0.9709 Time: 7.19566  | Val Loss: 0.4944 Accuracy: 0.8696\n",
      "Epoch: 143/150 Train Loss: 0.0972 Accuracy: 0.9681 Time: 7.14359  | Val Loss: 0.4934 Accuracy: 0.8696\n",
      "Epoch: 144/150 Train Loss: 0.1024 Accuracy: 0.9673 Time: 7.14588  | Val Loss: 0.4959 Accuracy: 0.8701\n",
      "Epoch: 145/150 Train Loss: 0.1003 Accuracy: 0.9682 Time: 7.18609  | Val Loss: 0.5006 Accuracy: 0.8670\n",
      "Epoch: 146/150 Train Loss: 0.1011 Accuracy: 0.9685 Time: 7.21013  | Val Loss: 0.4973 Accuracy: 0.8708\n",
      "Epoch: 147/150 Train Loss: 0.0948 Accuracy: 0.9702 Time: 7.17564  | Val Loss: 0.4926 Accuracy: 0.8690\n",
      "Epoch: 148/150 Train Loss: 0.1028 Accuracy: 0.9665 Time: 7.20152  | Val Loss: 0.5009 Accuracy: 0.8675\n",
      "Epoch: 149/150 Train Loss: 0.0982 Accuracy: 0.9691 Time: 7.24486  | Val Loss: 0.5004 Accuracy: 0.8652\n",
      "Epoch: 150/150 Train Loss: 0.0937 Accuracy: 0.9713 Time: 7.44851  | Val Loss: 0.4929 Accuracy: 0.8696\n",
      "#Parameter: 3228170 Accuracy: 0.8695541401273885\n"
     ]
    }
   ],
   "source": [
    "from hdd.models.cnn.mobilenet_v1 import MobileNetV1\n",
    "from hdd.train.classification_utils import (\n",
    "    naive_train_classification_model,\n",
    "    eval_image_classifier,\n",
    ")\n",
    "from hdd.models.nn_utils import count_trainable_parameter\n",
    "\n",
    "\n",
    "def train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    max_epochs=150,\n",
    ") -> tuple[MobileNetV1, dict[str, list[float]]]:\n",
    "    net = MobileNetV1(num_classes=10, width_multiplier=width_multiplier).to(DEVICE)\n",
    "    print(f\"#Parameter: {count_trainable_parameter(net)}\")\n",
    "    criteria = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, max_epochs, eta_min=lr / 100\n",
    "    )\n",
    "    training_stats = naive_train_classification_model(\n",
    "        net,\n",
    "        criteria,\n",
    "        max_epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        DEVICE,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return net, training_stats\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = build_dataloader(128, train_dataset, val_dataset)\n",
    "\n",
    "net, width_multiplier_1 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=1,\n",
    "    lr=0.01,\n",
    "    weight_decay=0,\n",
    "    max_epochs=150,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f41a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Parameter: 1832458\n",
      "Epoch: 1/150 Train Loss: 2.1558 Accuracy: 0.1941 Time: 6.08100  | Val Loss: 3.2276 Accuracy: 0.1692\n",
      "Epoch: 2/150 Train Loss: 1.9888 Accuracy: 0.2768 Time: 6.06480  | Val Loss: 2.0031 Accuracy: 0.2790\n",
      "Epoch: 3/150 Train Loss: 1.7359 Accuracy: 0.4074 Time: 6.06622  | Val Loss: 2.0154 Accuracy: 0.3761\n",
      "Epoch: 4/150 Train Loss: 1.5375 Accuracy: 0.4765 Time: 6.06678  | Val Loss: 1.5029 Accuracy: 0.5011\n",
      "Epoch: 5/150 Train Loss: 1.4244 Accuracy: 0.5238 Time: 6.08187  | Val Loss: 1.3914 Accuracy: 0.5287\n",
      "Epoch: 6/150 Train Loss: 1.3517 Accuracy: 0.5481 Time: 6.12345  | Val Loss: 1.2934 Accuracy: 0.5600\n",
      "Epoch: 7/150 Train Loss: 1.2755 Accuracy: 0.5813 Time: 6.03408  | Val Loss: 1.2936 Accuracy: 0.5778\n",
      "Epoch: 8/150 Train Loss: 1.2102 Accuracy: 0.5985 Time: 6.11045  | Val Loss: 1.0763 Accuracy: 0.6479\n",
      "Epoch: 9/150 Train Loss: 1.1512 Accuracy: 0.6188 Time: 6.04981  | Val Loss: 1.0718 Accuracy: 0.6420\n",
      "Epoch: 10/150 Train Loss: 1.1019 Accuracy: 0.6343 Time: 6.09722  | Val Loss: 1.1751 Accuracy: 0.6306\n",
      "Epoch: 11/150 Train Loss: 1.0448 Accuracy: 0.6630 Time: 6.14925  | Val Loss: 0.9619 Accuracy: 0.6999\n",
      "Epoch: 12/150 Train Loss: 0.9965 Accuracy: 0.6754 Time: 6.06964  | Val Loss: 0.9814 Accuracy: 0.6836\n",
      "Epoch: 13/150 Train Loss: 0.9629 Accuracy: 0.6927 Time: 6.10469  | Val Loss: 0.9270 Accuracy: 0.7080\n",
      "Epoch: 14/150 Train Loss: 0.9343 Accuracy: 0.6942 Time: 6.17763  | Val Loss: 0.9425 Accuracy: 0.7149\n",
      "Epoch: 15/150 Train Loss: 0.9127 Accuracy: 0.7043 Time: 6.13105  | Val Loss: 1.0532 Accuracy: 0.6749\n",
      "Epoch: 16/150 Train Loss: 0.8984 Accuracy: 0.7045 Time: 6.07935  | Val Loss: 0.8687 Accuracy: 0.7225\n",
      "Epoch: 17/150 Train Loss: 0.8482 Accuracy: 0.7244 Time: 6.23373  | Val Loss: 0.8262 Accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x756bacfac4a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tf/anaconda3/envs/pytorch-cu124/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 202067, 202099, 202131, 202163) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/multiprocessing/queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net, width_multiplier_75 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m eval_image_classifier(net, val_dataloader\u001b[38;5;241m.\u001b[39mdataset, DEVICE)\n\u001b[1;32m     11\u001b[0m ss \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mgt_label \u001b[38;5;241m==\u001b[39m result\u001b[38;5;241m.\u001b[39mpredicted_label \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_result]\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(train_dataloader, val_dataloader, width_multiplier, lr, weight_decay, max_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[1;32m     22\u001b[0m     net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(\n\u001b[1;32m     26\u001b[0m     optimizer, max_epochs, eta_min\u001b[38;5;241m=\u001b[39mlr \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m training_stats \u001b[38;5;241m=\u001b[39m \u001b[43mnaive_train_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net, training_stats\n",
      "File \u001b[0;32m~/workspace/hands-dirty-on-dl/hdd/train/classification_utils.py:156\u001b[0m, in \u001b[0;36mnaive_train_classification_model\u001b[0;34m(net, criteria, max_epochs, train_loader, val_loader, device, optimizer, scheduler, early_stopper, verbose, train_classifier, eval_classifier)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 156\u001b[0m avg_val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43meval_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/hands-dirty-on-dl/hdd/train/classification_utils.py:77\u001b[0m, in \u001b[0;36m_eval_classifier_naive\u001b[0;34m(net, criteria, val_loader, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cu124/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 202067, 202099, 202131, 202163) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "net, width_multiplier_75 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.75,\n",
    "    lr=0.01,\n",
    "    weight_decay=0,\n",
    "    max_epochs=150,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, width_multiplier_50 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.5,\n",
    "    lr=0.01,\n",
    "    weight_decay=0,\n",
    "    max_epochs=150,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, width_multiplier_25 = train_net(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    width_multiplier=0.25,\n",
    "    lr=0.01,\n",
    "    weight_decay=0,\n",
    "    max_epochs=150,\n",
    ")\n",
    "\n",
    "eval_result = eval_image_classifier(net, val_dataloader.dataset, DEVICE)\n",
    "ss = [result.gt_label == result.predicted_label for result in eval_result]\n",
    "print(f\"#Parameter: {count_trainable_parameter(net)} Accuracy: {sum(ss) / len(ss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
